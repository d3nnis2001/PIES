{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polygon import RESTClient\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489\n",
      "Downloaded data for 2023-01-01 00:00:00 to 2023-01-04 00:00:00\n",
      "1358\n",
      "Downloaded data for 2023-01-04 00:00:00 to 2023-01-07 00:00:00\n",
      "485\n",
      "Downloaded data for 2023-01-07 00:00:00 to 2023-01-10 00:00:00\n",
      "1384\n",
      "Downloaded data for 2023-01-10 00:00:00 to 2023-01-13 00:00:00\n",
      "461\n",
      "Downloaded data for 2023-01-13 00:00:00 to 2023-01-16 00:00:00\n",
      "1085\n",
      "Downloaded data for 2023-01-16 00:00:00 to 2023-01-19 00:00:00\n",
      "842\n",
      "Downloaded data for 2023-01-19 00:00:00 to 2023-01-22 00:00:00\n",
      "576\n",
      "Downloaded data for 2023-01-22 00:00:00 to 2023-01-25 00:00:00\n",
      "1022\n",
      "Downloaded data for 2023-01-25 00:00:00 to 2023-01-28 00:00:00\n",
      "419\n",
      "Downloaded data for 2023-01-28 00:00:00 to 2023-01-31 00:00:00\n",
      "1275\n",
      "Downloaded data for 2023-01-31 00:00:00 to 2023-02-03 00:00:00\n",
      "465\n",
      "Downloaded data for 2023-02-03 00:00:00 to 2023-02-06 00:00:00\n",
      "1384\n",
      "Downloaded data for 2023-02-06 00:00:00 to 2023-02-09 00:00:00\n",
      "902\n",
      "Downloaded data for 2023-02-09 00:00:00 to 2023-02-12 00:00:00\n",
      "820\n",
      "Downloaded data for 2023-02-12 00:00:00 to 2023-02-15 00:00:00\n",
      "1361\n",
      "Downloaded data for 2023-02-15 00:00:00 to 2023-02-18 00:00:00\n",
      "436\n",
      "Downloaded data for 2023-02-18 00:00:00 to 2023-02-21 00:00:00\n",
      "1384\n",
      "Downloaded data for 2023-02-21 00:00:00 to 2023-02-24 00:00:00\n",
      "465\n",
      "Downloaded data for 2023-02-24 00:00:00 to 2023-02-27 00:00:00\n",
      "1381\n",
      "Downloaded data for 2023-02-27 00:00:00 to 2023-03-02 00:00:00\n",
      "902\n",
      "Downloaded data for 2023-03-02 00:00:00 to 2023-03-05 00:00:00\n",
      "947\n",
      "Downloaded data for 2023-03-05 00:00:00 to 2023-03-08 00:00:00\n",
      "1363\n",
      "Downloaded data for 2023-03-08 00:00:00 to 2023-03-11 00:00:00\n",
      "506\n",
      "Downloaded data for 2023-03-11 00:00:00 to 2023-03-14 00:00:00\n",
      "1384\n",
      "Downloaded data for 2023-03-14 00:00:00 to 2023-03-17 00:00:00\n",
      "466\n",
      "Downloaded data for 2023-03-17 00:00:00 to 2023-03-20 00:00:00\n",
      "1384\n",
      "Downloaded data for 2023-03-20 00:00:00 to 2023-03-23 00:00:00\n",
      "882\n",
      "Downloaded data for 2023-03-23 00:00:00 to 2023-03-26 00:00:00\n",
      "967\n",
      "Downloaded data for 2023-03-26 00:00:00 to 2023-03-29 00:00:00\n",
      "1343\n",
      "Downloaded data for 2023-03-29 00:00:00 to 2023-04-01 00:00:00\n",
      "505\n",
      "Downloaded data for 2023-04-01 00:00:00 to 2023-04-04 00:00:00\n",
      "1343\n",
      "Downloaded data for 2023-04-04 00:00:00 to 2023-04-07 00:00:00\n",
      "45\n",
      "Downloaded data for 2023-04-07 00:00:00 to 2023-04-10 00:00:00\n",
      "1384\n",
      "Downloaded data for 2023-04-10 00:00:00 to 2023-04-13 00:00:00\n",
      "882\n",
      "Downloaded data for 2023-04-13 00:00:00 to 2023-04-16 00:00:00\n",
      "966\n",
      "Downloaded data for 2023-04-16 00:00:00 to 2023-04-19 00:00:00\n",
      "1343\n",
      "Downloaded data for 2023-04-19 00:00:00 to 2023-04-22 00:00:00\n",
      "505\n",
      "Downloaded data for 2023-04-22 00:00:00 to 2023-04-25 00:00:00\n",
      "1384\n",
      "Downloaded data for 2023-04-25 00:00:00 to 2023-04-28 00:00:00\n",
      "465\n",
      "Downloaded data for 2023-04-28 00:00:00 to 2023-05-01 00:00:00\n",
      "1340\n",
      "Downloaded data for 2023-05-01 00:00:00 to 2023-05-04 00:00:00\n",
      "882\n",
      "Downloaded data for 2023-05-04 00:00:00 to 2023-05-07 00:00:00\n",
      "966\n",
      "Downloaded data for 2023-05-07 00:00:00 to 2023-05-10 00:00:00\n",
      "1343\n",
      "Downloaded data for 2023-05-10 00:00:00 to 2023-05-13 00:00:00\n",
      "505\n",
      "Downloaded data for 2023-05-13 00:00:00 to 2023-05-16 00:00:00\n",
      "1384\n",
      "Downloaded data for 2023-05-16 00:00:00 to 2023-05-19 00:00:00\n",
      "467\n",
      "Downloaded data for 2023-05-19 00:00:00 to 2023-05-22 00:00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m start_date \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mTimestamp(\u001b[39m\"\u001b[39m\u001b[39m2023-01-01\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m csv_directory \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata_directory\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 57\u001b[0m data \u001b[39m=\u001b[39m downloadAllData(\u001b[39m\"\u001b[39;49m\u001b[39mC:XAUUSD\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mminute\u001b[39;49m\u001b[39m\"\u001b[39;49m, start_date, end_date, csv_directory)\n",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m, in \u001b[0;36mdownloadAllData\u001b[0;34m(ticker, timespan, start, end, csv_directory)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mwhile\u001b[39;00m current_start \u001b[39m<\u001b[39m end:\n\u001b[1;32m     29\u001b[0m     current_end \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(current_start \u001b[39m+\u001b[39m pd\u001b[39m.\u001b[39mDateOffset(days\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m), end)\n\u001b[0;32m---> 30\u001b[0m     data \u001b[39m=\u001b[39m getData(ticker, timespan, current_start, current_end)\n\u001b[1;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data\u001b[39m.\u001b[39mempty:\n\u001b[1;32m     33\u001b[0m         \u001b[39m# Create directory if it doesn't exist\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         os\u001b[39m.\u001b[39mmakedirs(csv_directory, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m, in \u001b[0;36mgetData\u001b[0;34m(ticker, timespan, start, end)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetData\u001b[39m(ticker, timespan, start, end):\n\u001b[0;32m----> 5\u001b[0m     bars \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mget_aggs(ticker\u001b[39m=\u001b[39;49mticker, multiplier\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, timespan\u001b[39m=\u001b[39;49mtimespan, from_\u001b[39m=\u001b[39;49mstart, to\u001b[39m=\u001b[39;49mend)\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(bars))\n\u001b[1;32m      9\u001b[0m     \u001b[39m#list of polygon OptionsContract objects to DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/polygon/rest/aggs.py:93\u001b[0m, in \u001b[0;36mAggsClient.get_aggs\u001b[0;34m(self, ticker, multiplier, timespan, from_, to, adjusted, sort, limit, params, raw, options)\u001b[0m\n\u001b[1;32m     90\u001b[0m     to \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(to\u001b[39m.\u001b[39mtimestamp() \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_mult(\u001b[39m\"\u001b[39m\u001b[39mmillis\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     91\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/v2/aggs/ticker/\u001b[39m\u001b[39m{\u001b[39;00mticker\u001b[39m}\u001b[39;00m\u001b[39m/range/\u001b[39m\u001b[39m{\u001b[39;00mmultiplier\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mtimespan\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mfrom_\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mto\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 93\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get(\n\u001b[1;32m     94\u001b[0m     path\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m     95\u001b[0m     params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_params(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_aggs, \u001b[39mlocals\u001b[39;49m()),\n\u001b[1;32m     96\u001b[0m     result_key\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mresults\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     97\u001b[0m     deserializer\u001b[39m=\u001b[39;49mAgg\u001b[39m.\u001b[39;49mfrom_dict,\n\u001b[1;32m     98\u001b[0m     raw\u001b[39m=\u001b[39;49mraw,\n\u001b[1;32m     99\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    100\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/polygon/rest/base.py:107\u001b[0m, in \u001b[0;36mBaseClient._get\u001b[0;34m(self, path, params, result_key, deserializer, raw, options)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m deserializer:\n\u001b[1;32m    106\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(obj) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m         obj \u001b[39m=\u001b[39m [deserializer(o) \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m obj]\n\u001b[1;32m    108\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         obj \u001b[39m=\u001b[39m deserializer(obj)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/polygon/rest/base.py:107\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m deserializer:\n\u001b[1;32m    106\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(obj) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m         obj \u001b[39m=\u001b[39m [deserializer(o) \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m obj]\n\u001b[1;32m    108\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         obj \u001b[39m=\u001b[39m deserializer(obj)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/polygon/rest/models/aggs.py:20\u001b[0m, in \u001b[0;36mAgg.from_dict\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_dict\u001b[39m(d):\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m Agg(\n\u001b[1;32m     21\u001b[0m         d\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mo\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m     22\u001b[0m         d\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mh\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m     23\u001b[0m         d\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39ml\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m     24\u001b[0m         d\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m     25\u001b[0m         d\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m     26\u001b[0m         d\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mvw\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m     27\u001b[0m         d\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m     28\u001b[0m         d\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m     29\u001b[0m         d\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39motc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m     30\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/polygon/modelclass.py:16\u001b[0m, in \u001b[0;36mmodelclass.<locals>.init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m (i, a) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(args):\n\u001b[1;32m     15\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(attributes):\n\u001b[0;32m---> 16\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[attributes[i]] \u001b[39m=\u001b[39m a\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m (k, v) \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m attributes:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "polygonAPIkey = 'nliMWVES2a1PaYt9tLC6DqdTSzBFaWXc'\n",
    "client = RESTClient(api_key=polygonAPIkey)\n",
    "\n",
    "def getData(ticker, timespan, start, end):\n",
    "    bars = client.get_aggs(ticker=ticker, multiplier=3, timespan=timespan, from_=start, to=end)\n",
    "    print(len(bars))\n",
    "\n",
    "\n",
    "    #list of polygon OptionsContract objects to DataFrame\n",
    "    downloadedData = pd.DataFrame(bars)\n",
    "\n",
    "    #create Date column\n",
    "    downloadedData['Date'] = pd.to_datetime(downloadedData['timestamp'], unit='ms')\n",
    "    downloadedData['Date'] = pd.to_datetime(downloadedData['Date'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    downloadedData.set_index('Date', inplace=True)\n",
    "\n",
    "    #drop unnecessary columns\n",
    "    downloadedData = downloadedData.drop(['vwap', 'transactions', 'otc'], axis=1)\n",
    "    downloadedData = downloadedData.rename(columns={\"open\": \"Open\", \"high\": \"High\", \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"})\n",
    "\n",
    "    return downloadedData\n",
    "\n",
    "\n",
    "def downloadAllData(ticker, timespan, start, end, csv_directory):\n",
    "    current_start = start\n",
    "    cumulative_data = pd.DataFrame()\n",
    "\n",
    "    while current_start < end:\n",
    "        current_end = min(current_start + pd.DateOffset(days=3), end)\n",
    "        data = getData(ticker, timespan, current_start, current_end)\n",
    "\n",
    "        if not data.empty:\n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(csv_directory, exist_ok=True)\n",
    "\n",
    "            # Save data for the current day\n",
    "            csv_file = os.path.join(csv_directory, f\"{current_start.strftime('%Y-%m-%d')}.csv\")\n",
    "            data.to_csv(csv_file)\n",
    "\n",
    "            # Append data to cumulative dataframe\n",
    "            cumulative_data = pd.concat([cumulative_data, data])\n",
    "\n",
    "            print(f\"Downloaded data for {current_start} to {current_end}\")\n",
    "\n",
    "        current_start = current_end + pd.DateOffset(days=0)\n",
    "\n",
    "    # Save cumulative data to a cumulative CSV file\n",
    "    cumulative_csv_file = os.path.join(csv_directory, \"idk.csv\")\n",
    "    cumulative_data.to_csv(cumulative_csv_file)\n",
    "\n",
    "    return cumulative_data\n",
    "\n",
    "end_date = pd.Timestamp(\"2023-06-01\")\n",
    "start_date = pd.Timestamp(\"2023-01-01\")\n",
    "csv_directory = \"data_directory\"\n",
    "\n",
    "data = downloadAllData(\"C:XAUUSD\", \"minute\", start_date, end_date, csv_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the directory path\n",
    "directory = './data_directory/'\n",
    "\n",
    "# Get a list of CSV files starting with the number 2\n",
    "csv_files = [file for file in os.listdir(directory) if file.startswith('2') and file.endswith('.csv')]\n",
    "\n",
    "# Sort the CSV files based on the dates in the file names\n",
    "csv_files = sorted(csv_files)\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each CSV file and concatenate the data\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    data = pd.read_csv(file_path)\n",
    "    combined_data = pd.concat([combined_data, data], ignore_index=True)\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "combined_data.to_csv('./data_directory/combined_data.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
